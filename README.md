# ğŸ“ˆ Real-Time Financial Market Anomaly Detection Platform

A production-like Big Data system that ingests free, public financial market data, processes it in real time with Spark Structured Streaming, detects price and volume anomalies using statistical methods, and visualizes results in Grafana.

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Apache Airflow (Orchestrator)                  â”‚
â”‚  â€¢ Monitors & auto-restarts all pipeline components                 â”‚
â”‚  â€¢ Validates data freshness, quality & load status                  â”‚
â”‚  â€¢ Verifies PostgreSQL + Parquet writes every 5 min                 â”‚
â”‚  â€¢ Hourly data quality checks & maintenance                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚   â”‚ Alpha Vantageâ”‚    â”‚  CoinGecko   â”‚    Data Sources              â”‚
â”‚   â”‚  (Stocks)    â”‚    â”‚  (Crypto)    â”‚                              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚          â”‚  REST API         â”‚  REST API                            â”‚
â”‚          â–¼                   â–¼                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚   â”‚ Stock        â”‚    â”‚ Crypto       â”‚    Ingestion                 â”‚
â”‚   â”‚ Producer     â”‚    â”‚ Producer     â”‚                              â”‚
â”‚   â”‚ (Python)     â”‚    â”‚ (Python)     â”‚                              â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚          â”‚                   â”‚                                      â”‚
â”‚          â–¼                   â–¼                                      â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚   â”‚          Apache Kafka           â”‚    Message Broker             â”‚
â”‚   â”‚  Topics:                        â”‚                               â”‚
â”‚   â”‚  â€¢ raw_stock_ticks              â”‚                               â”‚
â”‚   â”‚  â€¢ raw_crypto_ticks             â”‚                               â”‚
â”‚   â”‚  â€¢ clean_market_data            â”‚                               â”‚
â”‚   â”‚  â€¢ market_anomalies             â”‚                               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚                  â”‚                                                  â”‚
â”‚                  â–¼                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚   â”‚   Spark Structured Streaming    â”‚    Processing                 â”‚
â”‚   â”‚  â€¢ JSON parsing & schema        â”‚                               â”‚
â”‚   â”‚  â€¢ Window aggregations (5min)   â”‚                               â”‚
â”‚   â”‚  â€¢ Feature engineering          â”‚                               â”‚
â”‚   â”‚  â€¢ Z-score anomaly detection    â”‚                               â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚          â”‚              â”‚                                           â”‚
â”‚          â–¼              â–¼                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚   â”‚  Parquet   â”‚  â”‚ PostgreSQL  â”‚         Storage                   â”‚
â”‚   â”‚  (Local)   â”‚  â”‚ (Anomalies) â”‚                                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â”‚                          â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚    Grafana    â”‚         Visualization
                   â”‚  Dashboards   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

| Service | Technology | Purpose |
|---------|-----------|---------|
| **Kafka** | Confluent Kafka 7.5 | Message broker for real-time data streaming |
| **Zookeeper** | Confluent Zookeeper 7.5 | Kafka cluster coordination |
| **Stock Producer** | Python + kafka-python | Polls Alpha Vantage for stock data |
| **Crypto Producer** | Python + kafka-python | Polls CoinGecko for crypto data |
| **Spark** | Bitnami Spark 3.5 (Master + Worker) | Stream processing & anomaly detection |
| **PostgreSQL** | PostgreSQL 15 | Anomaly & clean data storage |
| **Grafana** | Grafana 10.2 | Real-time dashboards |
| **Airflow** | Apache Airflow 2.8.1 | Pipeline orchestration, scheduling & monitoring |

---

## ğŸ“Š Data Sources

All data sources are **free and public**. No paid API required.

### 1. Alpha Vantage (Stocks)
- **Endpoint**: `TIME_SERIES_INTRADAY` (5-minute intervals)
- **Symbols**: AAPL, MSFT, GOOGL, AMZN, TSLA (configurable)
- **Rate limit**: 5 calls/minute, 500 calls/day (free tier)
- **API Key**: Free at [alphavantage.co](https://www.alphavantage.co/support/#api-key)

### 2. CoinGecko (Cryptocurrencies)
- **Endpoints**: `/simple/price` and `/coins/{id}/ohlc`
- **Coins**: Bitcoin, Ethereum, Solana, Cardano, Ripple (configurable)
- **No API key required**
- **Rate limit**: ~50 calls/minute

---

## ğŸš€ How to Run

### Prerequisites

- **Docker** & **Docker Compose** installed
- **Alpha Vantage API key** (free): [Get one here](https://www.alphavantage.co/support/#api-key)

### Quick Start

```bash
# 1. Clone and navigate to the project
cd market-anomaly-platform

# 2. Create your .env file
cp .env.example .env
# Edit .env and add your Alpha Vantage API key

# 3. Start everything
docker-compose up -d

# 4. Check services are running
docker-compose ps

# 5. Open Grafana
# Navigate to http://localhost:3000 (admin/admin)
```

### Submit the Spark Job

Once the Spark cluster is running:

```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.1 \
  /opt/spark-app/streaming_job.py
```

### Service URLs

| Service | URL |
|---------|-----|
| Grafana | [http://localhost:3000](http://localhost:3000) |
| Airflow UI | [http://localhost:8082](http://localhost:8082) |
| Spark Master UI | [http://localhost:8080](http://localhost:8080) |
| Spark Worker UI | [http://localhost:8081](http://localhost:8081) |
| Kafka (external) | `localhost:29092` |
| PostgreSQL | `localhost:5432` |

### Stop & Clean Up

```bash
# Stop all services
docker-compose down

# Stop and remove all data volumes
docker-compose down -v
```

---

## ğŸ§  How Anomaly Detection Works

### Method: Z-Score Statistical Detection

The system uses the **Z-score method** to detect anomalies â€” a simple, interpretable statistical approach that requires no machine learning.

#### What is a Z-Score?

$$z = \frac{x - \mu}{\sigma}$$

Where:
- $x$ = current observed value
- $\mu$ = rolling mean of the feature
- $\sigma$ = rolling standard deviation

A Z-score tells you **how many standard deviations** a value is from the mean.

#### Detection Rules

| Condition | Interpretation |
|-----------|---------------|
| $\|z\| > 3$ | **Anomaly detected** (default threshold) |
| $\|z\| > 4$ | Medium severity |
| $\|z\| > 5$ | High severity |

#### Features Monitored

| Feature | Anomaly Type | Description |
|---------|-------------|-------------|
| **Price Return %** | `price_spike` | `(close - open) / open Ã— 100` â€” sudden price jumps |
| **Total Volume** | `volume_surge` | Aggregated volume in window â€” unusual trading activity |
| **Rolling Volatility** | `volatility_burst` | Stddev of price within window â€” market instability |

#### Processing Pipeline

1. **Raw ticks** arrive from Kafka (stock + crypto)
2. **Parsed & normalized** into a unified format with event-time
3. **5-minute window** aggregations compute OHLCV + features
4. **Rolling statistics** (mean, stddev) computed over a 20-window lookback per symbol
5. **Z-scores** calculated for each feature
6. **Anomalies flagged** when |z| > 3 and written to PostgreSQL

---

## ğŸ“Š Grafana Dashboards

Three pre-built dashboards are auto-provisioned when Grafana starts:

### 1. Market Overview
- Total anomalies detected (24h)
- Anomalies by type (pie chart)
- High severity count
- Anomaly timeline (hourly bar chart)
- Latest prices per symbol
- Anomalies by symbol ranking

### 2. Anomaly Alerts
- Live anomaly alert table (auto-refresh 10s)
- Color-coded severity (red/yellow/green)
- Z-score distribution histogram
- Anomalies per hour (stacked bars by severity)
- Top anomalous symbols
- Severity breakdown by asset type

### 3. Volatility Trends
- Rolling volatility over time (line chart per symbol)
- Price return % with threshold bands
- Volume delta bar chart
- Asset volatility comparison (latest snapshot)
- Volatility vs. Price Return scatter plot

### Grafana Credentials
- **Username**: `admin`
- **Password**: `admin` (change on first login)
- **PostgreSQL datasource**: auto-configured, named "MarketDB"

---

## ğŸ“ Project Structure

```
market-anomaly-platform/
â”œâ”€â”€ producers/
â”‚   â”œâ”€â”€ Dockerfile            # Docker image for Python producers
â”‚   â”œâ”€â”€ main.py               # Entrypoint â€“ routes to stock or crypto producer
â”‚   â”œâ”€â”€ config.py             # Centralized configuration (env vars)
â”‚   â”œâ”€â”€ stock_producer.py     # Alpha Vantage â†’ Kafka producer
â”‚   â””â”€â”€ crypto_producer.py    # CoinGecko â†’ Kafka producer
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ schemas.py            # PySpark schema definitions
â”‚   â””â”€â”€ streaming_job.py      # Spark Structured Streaming job
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ market_pipeline_dag.py   # Main pipeline orchestration DAG
â”‚   â”‚   â””â”€â”€ data_quality_dag.py      # Hourly data quality monitoring DAG
â”‚   â”œâ”€â”€ logs/                         # Airflow task logs (auto-generated)
â”‚   â””â”€â”€ plugins/                      # Custom Airflow plugins
â”œâ”€â”€ storage/
â”‚   â””â”€â”€ init.sql              # PostgreSQL schema initialization
â”œâ”€â”€ grafana/
â”‚   â”œâ”€â”€ provisioning/
â”‚   â”‚   â”œâ”€â”€ datasources/
â”‚   â”‚   â”‚   â””â”€â”€ datasource.yml    # Auto-provision PostgreSQL datasource
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â””â”€â”€ dashboards.yml    # Dashboard provider config
â”‚   â””â”€â”€ dashboards/
â”‚       â”œâ”€â”€ market_overview.json  # Market Overview dashboard
â”‚       â”œâ”€â”€ anomaly_alerts.json   # Anomaly Alerts dashboard
â”‚       â””â”€â”€ volatility_trends.json # Volatility Trends dashboard
â”œâ”€â”€ docker-compose.yml        # Full stack orchestration
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env.example              # Environment variable template
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                 # This file
```

---

## âš™ï¸ Configuration

All configuration is done via environment variables (`.env` file):

| Variable | Default | Description |
|----------|---------|-------------|
| `ALPHA_VANTAGE_API_KEY` | `demo` | Your Alpha Vantage API key |
| `STOCK_SYMBOLS` | `AAPL,MSFT,GOOGL,AMZN,TSLA` | Comma-separated stock tickers |
| `CRYPTO_IDS` | `bitcoin,ethereum,solana,cardano,ripple` | CoinGecko coin IDs |
| `STOCK_POLL_INTERVAL` | `60` | Seconds between stock API polls |
| `CRYPTO_POLL_INTERVAL` | `30` | Seconds between crypto API polls |
| `POSTGRES_USER` | `marketuser` | PostgreSQL username |
| `POSTGRES_PASSWORD` | `marketpass` | PostgreSQL password |
| `POSTGRES_DB` | `marketdb` | PostgreSQL database name |
| `GRAFANA_ADMIN_USER` | `admin` | Grafana admin username |
| `GRAFANA_ADMIN_PASSWORD` | `admin` | Grafana admin password |

---

## ï¿½ Airflow Orchestration

Apache Airflow manages and monitors the entire pipeline lifecycle through two DAGs:

### DAG 1: `market_anomaly_pipeline` (every 5 min)

End-to-end pipeline health monitoring and self-healing:

```
check_kafka â”€â”€â”
              â”œâ”€â–º verify_topics â”€â”€â”¬â”€â–º check_crypto_producer â”€â”€â”
check_postgresâ”˜                  â”œâ”€â–º check_stock_producer  â”€â”€â”€â”¼â”€â–º check_data_freshness â”€â”€â”¬â”€â–º check_row_counts â”€â”€â”
                                 â””â”€â–º check_spark_job â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â””â”€â–º check_kafka_lag â”€â”€â”€â”¼â”€â–º pipeline_summary
```

| Task | What it does |
|------|--------------|
| `check_kafka_health` | Verifies Kafka broker is responsive |
| `check_postgres_health` | Verifies PostgreSQL is accepting connections |
| `verify_kafka_topics` | Confirms all 4 required topics exist |
| `check_crypto_producer` | Validates producer is running; **auto-restarts** if down |
| `check_stock_producer` | Validates producer is running; **auto-restarts** if down |
| `check_or_submit_spark_job` | Detects if Spark job is running; **auto-submits** if not |
| `check_data_freshness` | Fails if no data in the last 15 minutes |
| `check_row_counts` | Logs row counts for clean_market_data and market_anomalies |
| `check_kafka_consumer_lag` | Reports consumer group lag |
| `pipeline_health_summary` | Prints per-symbol data summary |

### DAG 2: `data_quality_monitoring` (hourly)

Data quality checks and maintenance:

| Task | What it does |
|------|--------------|
| `check_null_values` | Scans for NULL values in critical columns |
| `check_duplicate_records` | Detects duplicate (symbol, window_start) groups |
| `check_price_sanity` | Validates price ranges aren't negative or extreme |
| `anomaly_statistics` | Reports anomaly counts by type and severity |
| `data_volume_trend` | Shows rows per window for the last hour |
| `check_parquet_output` | Reports Parquet storage size and file count |
| `cleanup_old_checkpoints` | Removes Spark checkpoints older than 24 hours |
| `quality_report_summary` | Aggregated data quality report |

### Airflow Credentials

- **URL**: [http://localhost:8082](http://localhost:8082)
- **Username**: `admin`
- **Password**: `admin`

---

## ï¿½ğŸ› ï¸ Troubleshooting

| Issue | Solution |
|-------|----------|
| Kafka not starting | Wait for Zookeeper health check to pass; check `docker logs kafka` |
| Alpha Vantage rate limit | Reduce `STOCK_SYMBOLS` or increase `STOCK_POLL_INTERVAL` |
| Spark job fails to connect to Kafka | Ensure Kafka is healthy: `docker-compose ps` |
| Grafana shows no data | Ensure Spark job is running and producing data to PostgreSQL |
| PostgreSQL connection refused | Check `docker logs postgres` for initialization errors |
| Airflow webserver not starting | Wait for `airflow-init` to complete; check `docker logs airflow-webserver` |
| Airflow DAGs not appearing | Verify files exist in `airflow/dags/`; check `docker logs airflow-scheduler` |
| Airflow DB errors | Airflow shares PostgreSQL; ensure `init.sql` doesn't conflict with Airflow tables |

---

## ğŸ“ License

This project is built for educational / portfolio purposes. Data sourced from free, public APIs.
